train_data <- df[train_index, ]
test_data <- df[-train_index, ]
bagging_model <- randomForest(HeartDisease ~ ., data = train_data, method = "class", ntree = 500)
test_pred <- predict(bagging_model, newdata = test_data, type = "class")
test_data$HeartDisease <- factor(test_data$HeartDisease, levels = c("0", "1"))
test_pred <- predict(tree_model, newdata = test_data, type = "class")
str(test_pred)
confusionMatrix(test_pred, test_data$HeartDisease)
plot(bagging_model)
boosting_model <- gbm(HeartDisease ~ .,
data = train_data,
distribution = "bernoulli",
n.trees = 500,
interaction.depth = 3,
shrinkage = 0.01,
cv.folds = 5,
verbose = FALSE)
summary(boosting_model)
boosting_predictions <- predict(boosting_model,
newdata = test_data,
n.trees = boosting_model$n.trees,
type = "response")
boosting_predictions_class <- ifelse(boosting_predictions > 0.5, "1", "0")
boosting_predictions_class <- factor(boosting_predictions_class, levels = c("0", "1"))
conf_matrix_boosting <- confusionMatrix(boosting_predictions_class, test_data$HeartDisease)
print(conf_matrix_boosting)
library(tidyverse)
library(caret)
library(dplyr)
library(corrplot)
library(reshape2)
library(MASS)
library(readr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(ggplot2)
library(factoextra)
df_scaled <- scale(df)
str(df)
df_numeric <- df[, sapply(df, is.numeric)]
df$Category <- as.numeric(as.factor(df$Category))
# Inspect the structure of the dataset
str(df)
# Select only numeric columns
df_numeric <- df[, sapply(df, is.numeric)]
# Convert non-numeric columns (e.g., "Category") to numeric, if needed
df$Category <- as.numeric(as.factor(df$Category))
# Check the structure of the dataset
str(df)
# Extract numeric columns
df_numeric <- df[, sapply(df, is.numeric)]
# Ensure 'Category' is numeric
if ("Category" %in% colnames(df) && !is.numeric(df$Category)) {
df$Category <- as.numeric(as.factor(df$Category))
}
# Add 'Category' back to df_numeric if necessary
if ("Category" %in% colnames(df)) {
df_numeric$Category <- df$Category
}
# Scale the numeric data
df_scaled <- scale(df_numeric)
# Perform PCA
pca_result <- prcomp(df_scaled, center = TRUE, scale. = TRUE)
# Display PCA summary
summary(pca_result)
# Visualize the explained variance
library(factoextra)
fviz_eig(pca_result)
# Extract the first two principal components
pca_data <- as.data.frame(pca_result$x[, 1:2])
# Check the structure of the dataset
#str(df)
# Extract numeric columns
df_numeric <- df[, sapply(df, is.numeric)]
# Ensure 'Category' is numeric
if ("Category" %in% colnames(df) && !is.numeric(df$Category)) {
df$Category <- as.numeric(as.factor(df$Category))
}
# Add 'Category' back to df_numeric if necessary
if ("Category" %in% colnames(df)) {
df_numeric$Category <- df$Category
}
# Scale the numeric data
df_scaled <- scale(df_numeric)
# Perform PCA
pca_result <- prcomp(df_scaled, center = TRUE, scale. = TRUE)
# Display PCA summary
summary(pca_result)
# Visualize the explained variance
library(factoextra)
fviz_eig(pca_result)
# Extract the first two principal components
pca_data <- as.data.frame(pca_result$x[, 1:2])
# Check the structure of the dataset
#str(df)
# Extract numeric columns
df_numeric <- df[, sapply(df, is.numeric)]
# Ensure 'Category' is numeric
if ("Category" %in% colnames(df) && !is.numeric(df$Category)) {
df$Category <- as.numeric(as.factor(df$Category))
}
# Add 'Category' back to df_numeric if necessary
if ("Category" %in% colnames(df)) {
df_numeric$Category <- df$Category
}
# Scale the numeric data
df_scaled <- scale(df_numeric)
# Perform PCA
pca_result <- prcomp(df_scaled, center = TRUE, scale. = TRUE)
# Display PCA summary
summary(pca_result)
# Visualize the explained variance
library(factoextra)
fviz_eig(pca_result)
# Extract the first two principal components
pca_data <- as.data.frame(pca_result$x[, 1:2])
df_numeric <- df[, sapply(df, is.numeric)]
if ("Category" %in% colnames(df) && !is.numeric(df$Category)) {
df$Category <- as.numeric(as.factor(df$Category))
}
if ("Category" %in% colnames(df)) {
df_numeric$Category <- df$Category
}
df_scaled <- scale(df_numeric)
pca_result <- prcomp(df_scaled, center = TRUE, scale. = TRUE)
summary(pca_result)
library(factoextra)
fviz_eig(pca_result)
pca_data <- as.data.frame(pca_result$x[, 1:2])
# Elbow method to find the optimal number of clusters
fviz_nbclust(pca_data, kmeans, method = "wss")  # Within-cluster sum of squares (WSS)
# Run K-Means with an appropriate k (e.g., k = 3)
set.seed(42)  =
kmeans_result <- kmeans(pca_data, centers = 3, nstart = 25)
# Elbow method to find the optimal number of clusters
fviz_nbclust(pca_data, kmeans, method = "wss")  # Within-cluster sum of squares (WSS)
# Run K-Means with an appropriate k (e.g., k = 3)
set.seed(42)
kmeans_result <- kmeans(pca_data, centers = 3, nstart = 25)
# Add cluster labels to PCA data
pca_data$Cluster <- as.factor(kmeans_result$cluster)
# Visualize clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = Cluster)) +
geom_point(size = 3) +
labs(title = "K-Means Clustering (PCA Reduced Data)", x = "PC1", y = "PC2")
# Elbow method to find the optimal number of clusters
fviz_nbclust(pca_data, kmeans, method = "wss")  # Within-cluster sum of squares (WSS)
# Run K-Means with an appropriate k (e.g., k = 3)
set.seed(42)
kmeans_result <- kmeans(pca_data, centers = 3, nstart = 25)
# Add cluster labels to PCA data
pca_data$Cluster <- as.factor(kmeans_result$cluster)
# Visualize clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = Cluster)) +
geom_point(size = 3) +
labs(title = "K-Means Clustering (PCA Reduced Data)", x = "PC1", y = "PC2")
fviz_nbclust(pca_data, kmeans, method = "wss")
set.seed(42)
kmeans_result <- kmeans(pca_data, centers = 3, nstart = 25)
pca_data$Cluster <- as.factor(kmeans_result$cluster)
ggplot(pca_data, aes(x = PC1, y = PC2, color = Cluster)) +
geom_point(size = 3) +
labs(title = "K-Means Clustering (PCA)", x = "PC1", y = "PC2")
table(pca_data$Cluster, df$Category)
# Add Cluster labels to the original dataframe
df$Cluster <- NA
df$Cluster[rownames(pca_data)] <- pca_data$Cluster
nrow(df)          # Number of rows in the original dataset
nrow(pca_data)    # Number of rows in the PCA-transformed dataset
length(cluster)   # Length of the cluster vector
# Add Cluster labels to the original dataframe
df$Cluster <- NA
df$Cluster[rownames(pca_data)] <- pca_data$Cluster
# Ensure pca_data contains principal components
pca_data <- prcomp(df[, -target_column], scale. = TRUE)$x[, 1:2]  # First 2 PCs
# Perform PCA, excluding the target column "HeartDisease"
pca_data <- prcomp(df[, !(names(df) %in% c("HeartDisease"))], scale. = TRUE)$x[, 1:2]
table(df$Cluster, df$HeartDisease)
# Add Cluster labels to the original dataframe
df$Cluster <- NA
df$Cluster[rownames(pca_data)] <- pca_data$Cluster
# Add clusters to the original dataframe
df$Cluster <- kmeans_result$cluster
# Contingency table: Clusters vs. HeartDisease
table(df$Cluster, df$HeartDisease)
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(Cluster))) +
geom_point(alpha = 0.6) +
labs(title = "K-Means Clustering (PCA Reduced Data)",
x = "PC1",
y = "PC2",
color = "Cluster") +
theme_minimal()
df$Cluster <- kmeans_result$cluster
table(df$Cluster, df$HeartDisease)
dist_matrix <- dist(pca_data[, 1:2])
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = FALSE, main = "Hierarchical Clustering Dendrogram")
cutree_result <- cutree(hc, k = 3)
pca_data$HC_Cluster <- as.factor(cutree_result)
ggplot(pca_data, aes(x = PC1, y = PC2, color = HC_Cluster)) +
geom_point(size = 3) +
labs(title = "Hierarchical Clustering (PCA Reduced Data)", x = "PC1", y = "PC2")
dist_matrix <- dist(pca_data[, 1:2])
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = FALSE, main = "Hierarchical Clustering Dendrogram")
cutree_result <- cutree(hc, k = 4)
pca_data$HC_Cluster <- as.factor(cutree_result)
ggplot(pca_data, aes(x = PC1, y = PC2, color = HC_Cluster)) +
geom_point(size = 3) +
labs(title = "Hierarchical Clustering (PCA Reduced Data)", x = "PC1", y = "PC2")
ggplot(pca_data, aes(x = PC1, y = PC2, color = HC_Cluster)) +
geom_point(size = 3) +
labs(title = "Hierarchical Clustering (PCA)", x = "PC1", y = "PC2")
dist_matrix <- dist(pca_data[, 1:2])
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = FALSE, main = "Hierarchical Clustering Dendrogram")
cutree_result <- cutree(hc, k = 2)
pca_data$HC_Cluster <- as.factor(cutree_result)
ggplot(pca_data, aes(x = PC1, y = PC2, color = HC_Cluster)) +
geom_point(size = 3) +
labs(title = "Hierarchical Clustering (PCA)", x = "PC1", y = "PC2")
dist_matrix <- dist(pca_data[, 1:2])
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = FALSE, main = "Hierarchical Clustering Dendrogram")
cutree_result <- cutree(hc, k = 4)
pca_data$HC_Cluster <- as.factor(cutree_result)
dist_matrix <- dist(pca_data[, 1:2])
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = FALSE, main = "Hierarchical Clustering Dendrogram")
cutree_result <- cutree(hc, k = 4)
pca_data$HC_Cluster <- as.factor(cutree_result)
ggplot(pca_data, aes(x = PC1, y = PC2, color = HC_Cluster)) +
geom_point(size = 3) +
labs(title = "Hierarchical Clustering (PCA)", x = "PC1", y = "PC2")
cutree_result <- cutree(hc, k = 3)
pca_data$HC_Cluster <- as.factor(cutree_result)
ggplot(pca_data, aes(x = PC1, y = PC2, color = HC_Cluster)) +
geom_point(size = 3) +
labs(title = "Hierarchical Clustering (PCA)", x = "PC1", y = "PC2")
cutree_result <- cutree(hc, k = 2)
pca_data$HC_Cluster <- as.factor(cutree_result)
ggplot(pca_data, aes(x = PC1, y = PC2, color = HC_Cluster)) +
geom_point(size = 3) +
labs(title = "Hierarchical Clustering (PCA)", x = "PC1", y = "PC2")
cutree_result <- cutree(hc, k = 2)
pca_data$HC_Cluster <- as.factor(cutree_result)
ggplot(pca_data, aes(x = PC1, y = PC2, color = HC_Cluster)) +
geom_point(size = 3) +
labs(title = "Hierarchical Clustering (PCA)", x = "PC1", y = "PC2")
cl4 <- cutree(hc, k = 4)
pca_data$HC_Cluster <- as.factor(cl4)
ggplot(pca_data, aes(x = PC1, y = PC2, color = HC_Cluster)) +
geom_point(size = 3) +
labs(title = "Hierarchical Clustering (PCA)", x = "PC1", y = "PC2")
cl3 <- cutree(hc, k = 3)
pca_data$HC_Cluster <- as.factor(cl3)
ggplot(pca_data, aes(x = PC1, y = PC2, color = HC_Cluster)) +
geom_point(size = 3) +
labs(title = "Hierarchical Clustering (PCA)", x = "PC1", y = "PC2")
cl2 <- cutree(hc, k = 2)
pca_data$HC_Cluster <- as.factor(cl2)
ggplot(pca_data, aes(x = PC1, y = PC2, color = HC_Cluster)) +
geom_point(size = 3) +
labs(title = "Hierarchical Clustering (PCA)", x = "PC1", y = "PC2")
sil2 <- silhouette(cl2, dist(df))
library(tidyverse)
library(caret)
library(dplyr)
library(corrplot)
library(reshape2)
library(MASS)
library(readr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(ggplot2)
library(factoextra)
library(cluster)
sil2 <- silhouette(cl2, dist(df))
sil3 <- silhouette(cl3, dist(df))
sil4 <- silhouette(cl4, dist(df))
mean(sil2[, 3])
mean(sil3[, 3])
mean(sil4[, 3])
df_numeric <- df[, sapply(df, is.numeric)]
sil2 <- silhouette(cl2, dist(df_numeric))
sil3 <- silhouette(cl3, dist(df_numeric))
sil4 <- silhouette(cl4, dist(df_numeric))
mean(sil2[, 3])
mean(sil3[, 3])
mean(sil4[, 3])
for (k in 2:6) {
cluster_labels <- cutree(hc, k = k)
silhouette_score <- mean(silhouette(cluster_labels, dist(df_scaled))[, 3])
print(paste("Clusters:", k, "Silhouette Score:", silhouette_score))
}
df_numeric <- df[, sapply(df, is.numeric)]
sil2 <- silhouette(cl2, dist(df_scaled))
sil3 <- silhouette(cl3, dist(df_scaled))
sil4 <- silhouette(cl4, dist(df_scaled))
mean(sil2[, 3])
mean(sil3[, 3])
mean(sil4[, 3])
sil2 <- silhouette(cl2, dist(df_scaled))
sil3 <- silhouette(cl3, dist(df_scaled))
sil4 <- silhouette(cl4, dist(df_scaled))
mean(sil2[, 3])
mean(sil3[, 3])
mean(sil4[, 3])
set.seed(123)
trainIndex <- createDataPartition(df$HeartDisease, p = 0.8, list = FALSE)
train_data <- df[trainIndex,]
test_data <- df[-trainIndex,]
tree_model <- rpart(HeartDisease ~ ., data = train_data, method = "class")
cp_values <- tree_model$cptable[,"CP"]
k <- 10
set.seed(123)
folds <- createFolds(train_data$HeartDisease, k = k)
cv_results <- sapply(cp_values, function(cp) {
fold_metrics <- sapply(folds, function(fold_indices) {
validation_data <- train_data[fold_indices,]
training_data <- train_data[-fold_indices,]
pruned_tree <- prune(tree_model, cp = cp)
val_pred <- predict(pruned_tree, validation_data, type = "class")
mean(val_pred == validation_data$HeartDisease)
})
mean(fold_metrics)
})
best_cp <- cp_values[which.max(cv_results)]
cat("Best cp:", best_cp, "\n")
final_model <- prune(tree_model, cp = best_cp)
testPred <- predict(final_model, newdata = test_data, type = "class")
testPred <- factor(testPred, levels = levels(test_data$HeartDisease))
test_data$HeartDisease <- factor(test_data$HeartDisease, levels = c("0", "1"))
testPred <- predict(tree_model, newdata = test_data, type = "class")
str(testPred)
confMatrix <- confusionMatrix(testPred, test_data$HeartDisease)
print(confMatrix)
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "https://cloud.r-project.org/"))
library(tidyverse)
library(caret)
library(dplyr)
library(corrplot)
library(reshape2)
library(MASS)
library(readr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(ggplot2)
library(factoextra)
library(cluster)
df <- read_csv("heart-disease.csv")
View(df)
colSums(is.na(df))
df$Sex <- as.factor(df$Sex)
df$ChestPainType <- as.factor(df$ChestPainType)
df$RestingECG <- as.factor(df$RestingECG)
df$ExerciseAngina <- as.factor(df$ExerciseAngina)
df$ST_Slope <- as.factor(df$ST_Slope)
numeric_data <- df %>%
mutate(
Sex = as.numeric(Sex),
ChestPainType = as.numeric(ChestPainType),
RestingECG = as.numeric(RestingECG),
ExerciseAngina = as.numeric(ExerciseAngina),
ST_Slope = as.numeric(ST_Slope)
)
cor_matrix <- cor(numeric_data)
melted_cor <- melt(cor_matrix)
ggplot(data = melted_cor, aes(Var1, Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Correlation Matrix Heatmap", x = "Attributes", y = "Attributes") +
geom_text(aes(label = round(value, 2)), color = "black", size = 2.5)
ggplot(df, aes(x = factor(HeartDisease), fill = Sex)) +
geom_bar(stat = "count") +
labs(title = "Gender Distribution by HeartDisease", x = "Heart Disease", y = "Count") +
scale_fill_manual(values = c("tomato", "lightblue"))
ggplot(df, aes(x = factor(HeartDisease), y = Age)) +
geom_boxplot(fill = "tomato") +
labs(title = "Age Distribution by HeartDisease", x = "Heart Disease", y = "Age")
ggplot(df, aes(x = factor(HeartDisease), fill = ExerciseAngina)) +
geom_bar(stat = "count") +
labs(title = "ExerciseAngina Distribution by HeartDisease", x = "Heart Disease", y = "Count") +
scale_fill_manual(values = c("tomato", "lightblue"))
ggplot(df, aes(x = factor(HeartDisease), fill = ST_Slope)) +
geom_bar(stat = "count") +
labs(title = "ST_Slope Distribution by HeartDisease", x = "Heart Disease", y = "Count") +
scale_fill_manual(values = c("tomato", "lightblue", "green"))
set.seed(123)
trainIndex <- createDataPartition(df$HeartDisease, p = 0.8, list = FALSE)
train_data <- df[trainIndex,]
test_data <- df[-trainIndex,]
tree_model <- rpart(HeartDisease ~ ., data = train_data, method = "class")
cp_values <- tree_model$cptable[,"CP"]
k <- 10
set.seed(123)
folds <- createFolds(train_data$HeartDisease, k = k)
cv_results <- sapply(cp_values, function(cp) {
fold_metrics <- sapply(folds, function(fold_indices) {
validation_data <- train_data[fold_indices,]
training_data <- train_data[-fold_indices,]
pruned_tree <- prune(tree_model, cp = cp)
val_pred <- predict(pruned_tree, validation_data, type = "class")
mean(val_pred == validation_data$HeartDisease)
})
mean(fold_metrics)
})
best_cp <- cp_values[which.max(cv_results)]
cat("Best cp:", best_cp, "\n")
final_model <- prune(tree_model, cp = best_cp)
testPred <- predict(final_model, newdata = test_data, type = "class")
testPred <- factor(testPred, levels = levels(test_data$HeartDisease))
test_data$HeartDisease <- factor(test_data$HeartDisease, levels = c("0", "1"))
testPred <- predict(tree_model, newdata = test_data, type = "class")
str(testPred)
confMatrix <- confusionMatrix(testPred, test_data$HeartDisease)
print(confMatrix)
rpart.plot(
tree_model,
type = 2,
extra = 104,
under = TRUE,
fallen.leaves = TRUE,
cex = 0.7,
box.palette = "RdYlGn",
shadow.col = "gray",
main = "Decision Tree"
)
set.seed(123)
train_index <- createDataPartition(y = df$HeartDisease, p = 0.8, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
bagging_model <- randomForest(HeartDisease ~ ., data = train_data, method = "class", ntree = 500)
test_pred <- predict(bagging_model, newdata = test_data, type = "class")
test_data$HeartDisease <- factor(test_data$HeartDisease, levels = c("0", "1"))
test_pred <- predict(tree_model, newdata = test_data, type = "class")
str(test_pred)
confusionMatrix(test_pred, test_data$HeartDisease)
plot(bagging_model)
boosting_model <- gbm(HeartDisease ~ .,
data = train_data,
distribution = "bernoulli",
n.trees = 500,
interaction.depth = 3,
shrinkage = 0.01,
cv.folds = 5,
verbose = FALSE)
summary(boosting_model)
boosting_predictions <- predict(boosting_model,
newdata = test_data,
n.trees = boosting_model$n.trees,
type = "response")
boosting_predictions_class <- ifelse(boosting_predictions > 0.5, "1", "0")
boosting_predictions_class <- factor(boosting_predictions_class, levels = c("0", "1"))
conf_matrix_boosting <- confusionMatrix(boosting_predictions_class, test_data$HeartDisease)
print(conf_matrix_boosting)
df_numeric <- df[, sapply(df, is.numeric)]
if ("Category" %in% colnames(df) && !is.numeric(df$Category)) {
df$Category <- as.numeric(as.factor(df$Category))
}
if ("Category" %in% colnames(df)) {
df_numeric$Category <- df$Category
}
df_scaled <- scale(df_numeric)
pca_result <- prcomp(df_scaled, center = TRUE, scale. = TRUE)
summary(pca_result)
fviz_eig(pca_result)
pca_data <- as.data.frame(pca_result$x[, 1:2])
fviz_nbclust(pca_data, kmeans, method = "wss")
set.seed(42)
kmeans_result <- kmeans(pca_data, centers = 3, nstart = 25)
pca_data$Cluster <- as.factor(kmeans_result$cluster)
ggplot(pca_data, aes(x = PC1, y = PC2, color = Cluster)) +
geom_point(size = 3) +
labs(title = "K-Means Clustering (PCA)", x = "PC1", y = "PC2")
df$Cluster <- kmeans_result$cluster
table(df$Cluster, df$HeartDisease)
dist_matrix <- dist(pca_data[, 1:2])
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = FALSE, main = "Hierarchical Clustering Dendrogram")
cl4 <- cutree(hc, k = 4)
pca_data$HC_Cluster <- as.factor(cl4)
ggplot(pca_data, aes(x = PC1, y = PC2, color = HC_Cluster)) +
geom_point(size = 3) +
labs(title = "Hierarchical Clustering (PCA)", x = "PC1", y = "PC2")
cl3 <- cutree(hc, k = 3)
pca_data$HC_Cluster <- as.factor(cl3)
ggplot(pca_data, aes(x = PC1, y = PC2, color = HC_Cluster)) +
geom_point(size = 3) +
labs(title = "Hierarchical Clustering (PCA)", x = "PC1", y = "PC2")
cl2 <- cutree(hc, k = 2)
pca_data$HC_Cluster <- as.factor(cl2)
ggplot(pca_data, aes(x = PC1, y = PC2, color = HC_Cluster)) +
geom_point(size = 3) +
labs(title = "Hierarchical Clustering (PCA)", x = "PC1", y = "PC2")
sil2 <- silhouette(cl2, dist(df_scaled))
sil3 <- silhouette(cl3, dist(df_scaled))
sil4 <- silhouette(cl4, dist(df_scaled))
mean(sil2[, 3])
mean(sil3[, 3])
mean(sil4[, 3])
setwd("C:/Users/User/Desktop/FALL 2024/CSC 463/phase 3")
setwd("C:/Users/User/Desktop/FALL 2024/CSC 463/phase 3")
