# Heart-Disease-Risk-Prediction
# Dataset Description
Link to the dataset: https://www.kaggle.com/datasets/amiramohammedi07/heart-disease-prediction

The Heart Disease dataset provides critical information about patients making it useful to predict the likelihood of heart disease, identify risk factors, and gain insights into cardiovascular health trends. 

#  Data mining tasks: data preprocessing, visualization, and exploration techniques
- To better understand the dataset, we start by checking for missing values in each column, then we convert categorical factors into numeric values and compute the correlation matrix.
- Results:  The relations between ExerciseAngina and HeartDisease, ST_Slope and HeartDisease, and ST_Slope and Oldpeak seem worth investigating

# Supervised Learning: Decision Trees, Random Forest, Bagging and Boosting
- Decision tres: the accuracy of 85.25% and balanced accuracy of 84.90% indicate that the model performs well on both classes without significant bias. The relatively high specificity (89.80%) suggests the model effectively identifies individuals with heart disease. As for error distribution, the proportion of false positives and false negatives is not significantly different, suggesting balanced error distribution which is indicated by the p-value of 0.2482 (McNemar Test). The top predictors were found to be ST_Slope and ChestPainType in this model, consistent with their importance in stress test results and clinical presentations of heart disease. The decision tree also highlights interactions, such as how specific ChestPainType values and Oldpeak interact to influence predictions.
- Random forest and bagging: the confusion matrix shows that the model predicts a significant number of positives and negatives correctly. The relatively high sensitivity and specificity indicate that the model is well-balanced, performing well at identifying both heart disease (positive class) and non-heart disease (negative class) cases. 
- Boosting:  The boosting model performs well with a high accuracy of 89.62% and good specificity and sensitivity, indicating it is effective at identifying both classes. Feature importance reveals that Cholesterol, MaxHR, and RestingECG are crucial features in predicting heart disease, with Cholesterol being the most influential. This model is well-tuned for the dataset and shows promising results in predicting heart disease risk.

# Unsupervised Learning: K-means, hierarchical clustering and PCA
- PCA analysis: The first principal component PC1 explains about 31.8% of the variance, followed by PC2 with 18.2%. Together, PC1 and PC2 explain about 50.0% of the variance. From PC3 onward, each component contributes less to explaining the variance. Since we are looking for dimension reduction for clustering and not for accuracy, we only extract the first two principal components for K-means and Hierarchical clustering.
- K-means analysis: The results show that clusters 1 and 2 seem to have a majority of people with heart disease while cluster 3 has a majority of people without heart disease. Cluster 3 is relatively pure as most individuals in it (343 out of 396) do not have heart disease, while clusters 1 and 2 have a mix with a majority towards class “1”. Therefore, we can label cluster 3 as “healthy patients”. As for clusters 1 and 2, further analysis is needed to explore the characteristics of each cluster and understand better their trends.
- Hierarchical clustering: The silhouette scores suggest that clusters 2 and 3 may be the best fits. However, these scores are still low knowing that a good silhouette score is 1. This could be due to the nature of our dataset that has complex relationships or overlapping clusters, making it difficult to separate clearly. K-means and hierarchical clustering assume that clusters are spherical. Therefore, these algorithms may not perform well for more complex shapes. Possible solutions include feature selection and change of distance metric.

# Results and Final Comments:
To sum up, this project explored both supervised and unsupervised machine learning techniques to predict and analyze heart disease risk.
 
**Supervised Learning**: The decision tree (DT), random forest (RF), and boosting models were evaluated for their ability to predict heart disease. Among these, boosting emerged as the most accurate model, achieving an accuracy of 89.62% and a balanced accuracy of 89.29%, with Cholesterol, MaxHR, and RestingECG identified as the most critical features. Random Forest and Decision Tree models performed similarly, with accuracies of 85.25%. The random forest, however, demonstrated higher robustness through its ensemble approach. The DT model offered interpretability by highlighting key factors such as ST_Slope and ChestPainType, which align with established medical knowledge, validating the models’ clinical relevance. Finally, the McNemar’s Test confirmed that all supervised models maintained a balanced error distribution, and their high kappa statistics indicated good agreement between predictions and actual labels.
 
**Unsupervised Learning**: Unsupervised techniques such as PCA, k-means, and hierarchical clustering provided valuable insights into the data structure: PCA revealed that the first two components explained 50% of the variance, enabling dimensionality reduction for clustering tasks while K-means clustering suggested three natural groupings in the data, with one cluster predominantly representing healthy patients and two clusters reflecting a mix of individuals with heart disease. Finally, hierarchical clustering supported the findings from k-means but exhibited challenges in achieving high silhouette scores, highlighting potential overlaps and complex relationships in the data.
 
**Future Directions**: While the models demonstrated strong performance, future improvements could focus on exploring advanced clustering methods, such as DBSCAN or Gaussian Mixture Models, to address nonspherical data distributions and applying feature engineering techniques to enhance model interpretability and performance.
